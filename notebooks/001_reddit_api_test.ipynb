{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddit: datascience\n",
      "Suscriptores: 2556782\n"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit('datascience')\n",
    "print(f\"Subreddit: {subreddit.display_name}\")\n",
    "print(f\"Suscriptores: {subreddit.subscribers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1hx305z', 'title': 'I was penalized in a DS interview for answering that I would use a Generalized Linear Model for an A/B test with an outcome of time on an app... But a linear model with a binary predictor is equivalent to a t-test. Has anyone had occasions where the interviewer was wrong?', 'author': 'Stauce52', 'subreddit': 'datascience', 'upvotes': 267, 'created_utc': 1736391735.0, 'text': \"Hi,\\n\\nI underwent a technical interview for a DS role at a company. The company was nice enough to provide feedback. This reason was not only reason I was rejected, but I wanted to share because it was very surprising to me. \\n\\nThey said I aced the programming. However, hey gave me feedback that my statistics performance was mixed. I was surprised. The question was what type of model would I use for an A/B test with time spent on an app as an outcome. I suspect many would use a t-test but I believe that would be inappropriate since time is a skewed outcome, with only positive values, so a t-test would not fit the data well (i.e., Gaussian outcome). I suggested a log-normal or log-gamma generalized linear model instead.\\n\\n  \\nI later received feedback that I was penalized for suggesting a linear model for the A/B test. However, a linear model with a binary predictor *is equivalent to a t-test*. I don't want to be arrogant or presumptuous that I think the interviewer is wrong and I am right, but I am struggling to have any other interpretation than the interviewer did not realize a linear model with a binary predictor is equivalent to a t-test.\\n\\nHas anyone else had occasions in DS interviewers where the interviewer may have misunderstood or been wrong in their assessment?\"}\n",
      "{'id': '1hxalxo', 'title': 'Companies are finally hiring', 'author': 'mediocrity4', 'subreddit': 'datascience', 'upvotes': 1563, 'created_utc': 1736421422.0, 'text': 'I applied to 80+ jobs before the new year and got rejected or didn’t hear back from most of them. A few positions were a level or two lower than my currently level. I got only 1 interview and I did accept the offer. \\n\\nIn the last week, 4 companies reached out for interviews. Just want to put this out there for those who are still looking. Keep going at it. \\n\\nEdit - thank you all for the congratulations and I’m sorry I can’t respond to DMs. Here are answers to some common questions. \\n\\n1. The technical coding challenge was only SQL. Frankly in my 8 years of analytics, none of my peers use Python regularly unless their role is to automate or data engineering. You’re better off mastering SQL by using leetcode and DataLemur\\n\\n2. Interviews at all the FAANGs are similar. Call with HR rep, first round is with 1 person and might be technical. Then a final round with a bunch of individual interviews on the same day. Most of the questions will be STAR format. \\n\\n3. As for my skillsets, I advertise myself as someone who can build strategy, project manage, and can do deep dive analyses. I’m never going to compete against the recent grads and experts in ML/LLM/AI on technical skills, that’s just an endless grind to stay at the top. I would strongly recommend others to sharpen their soft skills. A video I watched recently is from The Diary of a CEO with Body Language Expert with Vanessa Edwards. I legit used a few tips during my interviews and I thought that helped '}\n",
      "{'id': '1hxi5em', 'title': '[R][N] TabPFN v2: Accurate predictions on small data with a tabular foundation model', 'author': 'Mysterious-Rent7233', 'subreddit': 'datascience', 'upvotes': 7, 'created_utc': 1736443855.0, 'text': ''}\n",
      "{'id': '1hxnq3t', 'title': 'Question on quasi-experimental approach for product feature change measurement', 'author': 'Deray22', 'subreddit': 'datascience', 'upvotes': 5, 'created_utc': 1736457945.0, 'text': 'I work in ecommerce analytics and my team runs dozens of traditional, \"clean\" online A/B tests each year. That said, I\\'m far from an expert in the domain - I\\'m still working through a part-time master\\'s degree and I\\'ve only been doing experimentation (without any real training) for the last 2.5 years. \\n\\nOne of my product partners wants to run a learning test to help with user flow optimization. But because of some engineering architecture limitations, we can\\'t do a normal experiment. Here are some details:\\n\\n* Desired outcome is to understand the impact of removing the (outdated) new user onboarding flow in our app. \\n* Proposed approach is to release a new app version without the onboarding flow and compare certain engagement, purchase, and retention outcomes.\\n* \"Control\" group: users in the previous app version who did experience the new user flow\\n* \"Treatment\" group: users in the new app version who *would have* gotten the new user flow had it not been removed\\n\\nOne major thing throwing me off is how to handle the shifted time series; the 4 weeks of data I\\'ll look at for each group will be different time periods. Another thing is the lack of randomization, but that can\\'t be helped.\\n\\nGiven these parameters, curious what might be the best way to approach this type of \"test\"? My initial thought was to use difference-in-difference but I don\\'t think it applies given the specific lack of \\'before\\' for each group. '}\n",
      "{'id': '1hxplq8', 'title': 'Best resources for CO2 emissions modeling forecasting', 'author': 'Corpulos', 'subreddit': 'datascience', 'upvotes': 9, 'created_utc': 1736462904.0, 'text': \"I'm looking for a good textbook or resource to learn about air emissions data modeling and forecasting using statistical methods and especially machine learning. Also, can you discuss your work in the field; id like tonlearn more.\"}\n",
      "{'id': '1hxt0wl', 'title': 'How good are your linear algebra skills?', 'author': 'officialcrimsonchin', 'subreddit': 'datascience', 'upvotes': 86, 'created_utc': 1736472663.0, 'text': 'Started my masters in computer science in August. Bachelors was in chemistry so I took up to diff eq but never a full linear algebra class. I’m still familiar with a lot of the concepts as they are used in higher level science classes, but in my machine learning class I’m kind of having to teach myself a decent bit as I go. Maybe it’s me over analyzing and wanting to know the deep concepts behind everything I learn, and I’m sure in the real world these pure mathematical ideas are rarely talked about, but I know having a strong understanding of core concepts of a field help you succeed in that field more naturally as it begins becoming second nature.\\n\\nShould I lighten my course load to take a linear algebra class or do you think my basic understanding (although not knowing how basic that is) will likely be good enough?'}\n",
      "{'id': '1hxxjz6', 'title': \"Microsoft's rStar-Math: 7B LLMs matches OpenAI o1's performance on maths\", 'author': 'mehul_gupta1997', 'subreddit': 'datascience', 'upvotes': 3, 'created_utc': 1736487671.0, 'text': ''}\n",
      "{'id': '1hy7g0m', 'title': 'SQL Squid Game: Imagine you were a Data Scientist for Squid Games (9 Levels)', 'author': 'NickSinghTechCareers', 'subreddit': 'datascience', 'upvotes': 527, 'created_utc': 1736524298.0, 'text': ''}\n",
      "{'id': '1hy8jhq', 'title': 'SAS - SQL question: inobs= vs outobs=', 'author': 'berserk539', 'subreddit': 'datascience', 'upvotes': 5, 'created_utc': 1736527127.0, 'text': \"Just a quick question here regarding PROC SQL in SAS.  Let's say I'm just writing some code and I want to test it.  Since the database I'm querying has over a million records, I don't want it to process my code for all the records.  \\n\\nMy understanding is that I would want to use the inobs= option to limit how much of the table is queried and processed on the server.  Is this correct?\\n\\nThe outobs= option will return however many records I set, but it process every record on the table in the server.  Is this correct?\"}\n",
      "{'id': '1hy9am1', 'title': 'Spreadsheet first cell debate ', 'author': 'clashofphish', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1736529004.0, 'text': \"Settle this debate I'm having with a coworker. \\n\\nI say that spreadsheets should always start in row 1, column A. They say row 2, column B, [edit] so that there is an empty row and column before the table starts.\\n\\nWhat's your take?\"}\n",
      "{'id': '1hyaw2t', 'title': 'How to communicate with investors?', 'author': 'AdministrativeRub484', 'subreddit': 'datascience', 'upvotes': 16, 'created_utc': 1736532961.0, 'text': 'I\\'m working at a small scale startup and my CEO is always in talks with investors apparently. I\\'m currently working in different architectures for video classification as well as using large multimodal models to classify video. They want to show how no other model works on our own data (obviously) and how recent architectures are not as good as our own super secret model (videoMAE finetunned on our data...). I\\'m okay with faking results/showing results that cannot be compared fairly. I mean I\\'m not but if that\\'s what they want to do then fine, doesn\\'t really involve more work for me.\\n\\nNow what pisses me off is that now I need to come up with a way to get an accuracy per class in a multilabel classification setting based solely on precision and recall per class because different models were evaluated by different people at different times and I really only have those 2 metrics per class - precision and recall. I don\\'t even know if this is possible, it feels like it isn\\'t, and is an overall dumb metric for our use case. All because investors only know the word \"accuracy\"....\\n\\nWould it not be enough to say: \"This is the F1 score for our most important classes, and as you can see, none of the other models or architectures we\\'ve tried are as good as our best model... By the way, if you don\\'t know what F1 means, just know that higher scores are better. If you want, I can explain it in more detail...\" as opposed to getting metrics that do not make any sense...?\\n\\nI will not present it to the investors, I only need to come up with a document, but wouldn\\'t it be enough for the higher ups in my company to say what I said above in this scenario? '}\n",
      "{'id': '1hyhm2a', 'title': 'Is it necessary to understand the mathematics for data science anymore?', 'author': 'Other-Economy8403', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1736550108.0, 'text': 'The general consensus has been that you need to know the maths behind the models (proofs) in data science and that it’s advantageous to do so. But in this era of LLMs making our work even easier, and all the tools we use having already baked in the math behind the models for us, I wonder if this statement remains true or if it’s outdated advice. For example, in my limited experience of doing DS work, I’m personally yet to come across a situation where I was able to debug something because I knew the deep math proofs behind it (I did stats so know a decent amount of proofs). But I’m also very new to DS work so perhaps I’m missing something. \\n\\nObviously understanding model output and what each of them means such as AUC, residuals, checking for drift etc remains important and will always do so.'}\n",
      "{'id': '1hyploh', 'title': '200 applications - no response, please help. I have applied for data science (associate or mid-level) positions. Thank you ', 'author': 'Sad_Campaign713', 'subreddit': 'datascience', 'upvotes': 427, 'created_utc': 1736575525.0, 'text': ''}\n",
      "{'id': '1hyte5x', 'title': 'Feeling stuck in my career. Please help', 'author': 'SemperZero', 'subreddit': 'datascience', 'upvotes': 58, 'created_utc': 1736592284.0, 'text': \"I'm in a weird position, where I feel like I'm stuck in my career. I really enjoy mathematics, ML/AI, implemented a lot of algorithms from scratch in C, developed new models for business purposes, presented at some internal/small conferences, and developed entire ML infrastructures for startups, but having no real opportunities to grow more.\\n\\nAt the moment I'm making over 100k$ working remotely from eastern Europe for a FAANG in the US (they have an office here, but my entire data science team is based in the US and I'm working on the same things as them).\\n\\nWhen applying to companies in the US/UK I'm receiving zero callbacks (willing to relocate), although companies from the same areas are reaching out with remote offers of \\\\~100k$/year. Those don't have the benefits of my current company, and are not attractive opportunities. I'm looking to relocate and get 200k$+. Current internal transfers to the US are closed, as they are looking to expand in east Europe. I've also asked for more difficult projects, but those are only available for US, not for my region.\\n\\nThe projects that are open to me at the moment offer zero satisfaction and I want to solve more complex problems and continue to expand my skills, but I'm stuck for the only thing that my studies are in eastern Europe and that I don't hold a PhD, even though I've already worked on novel models in industry, and speaking with friends and colleagues that hold a PhD, my skills are on par.\\n\\nI'm at a point where I feel like skills and projects don't mean absolutely anything, and the only thing that has any weight for getting a job are diplomas and people you know... Maybe I'm exaggerating, but from all of my experiences I'm starting to feel like people from my region without studies abroad are seen only as cheap labor that should never be given the chance to work on real problems and be paid accordingly (a shitty company directly told me that, while another told me explicitly that my skills don't matter and they're only offering bad projects with bad pay in my region). It's like, there's a limit to the level of difficulty I can work on and the pay I can receive, regardless of how much I outcompete others...\\n\\nAt the moment, I'm working on a side research project that I'll be sending to some top tier conferences, and then try getting a PhD in the west... but that will take years, and if I already have the skills it's so frustrating to be stuck for so long just for a diploma and a title...\\n\\nOr maybe my skills are really not on par, and I'm only good compared to the people in my region? Here's my resume if anyone would be willing to offer me some feedback.\"}\n",
      "{'id': '1hyxec6', 'title': 'Simple Full stack Agentic AI project to please your Business stakeholders', 'author': 'takuonline', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1736606856.0, 'text': 'Since you all refused to share how you are applying gen ai in the real world, I figured I would just share mine.\\n\\n  \\nSo here it is:  [https://adhoc-insights.takuonline.com/](https://adhoc-insights.takuonline.com/)  \\nThere is a rate limiter, but we will see how it goes.\\n\\n\\n\\nTech Stack:\\n\\nFrontend: Next.js, Tailwind, shadcn\\n\\nBackend: Django (DRF), langgraph\\n\\nLLM: Claude 3.5 Sonnet\\n\\nI am still unsure if l should sell it as a tool for data analysts that makes them more productive or for quick and easy data analysis for business stakeholders to self-serve on low-impact metrics.\\n\\nSo what do you all think?'}\n",
      "{'id': '1hzpcuv', 'title': 'How we matured Fisher, our A/B testing library', 'author': 'chomoloc0', 'subreddit': 'datascience', 'upvotes': 64, 'created_utc': 1736696534.0, 'text': ''}\n",
      "{'id': '1i03pk7', 'title': 'Where do you go to stay up to date on data analytics/science?', 'author': 'lowkeyripper', 'subreddit': 'datascience', 'upvotes': 304, 'created_utc': 1736735034.0, 'text': 'Are there any people or organizations you follow on Youtube, Twitter, Medium, LinkedIn, or some other website/blog/podcast that you always tend to keep going back to? \\n\\nMy previous career absolutely lacked all the professional \"content creators\" that data analytics have, so I was wondering what content you guys tend to consume, if any. Previously I\\'d go to two sources: one to stay up to date on semi-relevant news, and the other was a source that\\'d do high level summaries of interesting research papers. \\n\\nReally, the kind of stuff would be talking about new tools/products that might be of use, tips and tricks, some re-learning of knowledge you might have learned 10+ years ago, deep dives of random but pertinent topics, or someone that consistently puts out unique visualizations and how to recreate them. You can probably see what I\\'m getting at: sources for stellar information.'}\n",
      "{'id': '1i06k3y', 'title': 'Weekly Entering & Transitioning - Thread 13 Jan, 2025 - 20 Jan, 2025', 'author': 'AutoModerator', 'subreddit': 'datascience', 'upvotes': 6, 'created_utc': 1736744505.0, 'text': \" \\n\\nWelcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\\n\\n* Learning resources (e.g. books, tutorials, videos)\\n* Traditional education (e.g. schools, degrees, electives)\\n* Alternative education (e.g. online courses, bootcamps)\\n* Job search questions (e.g. resumes, applying, career prospects)\\n* Elementary questions (e.g. where to start, what next)\\n\\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\"}\n",
      "{'id': '1i0bhi3', 'title': 'Seeking Advice on GPU Comparison: GreenNode vs FPT', 'author': 'jameslee2295', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1736765919.0, 'text': 'I’m currently exploring GPU options for my projects and I’m curious if anyone here has experience using GPUs from GreenNode or FPT. I’m looking for real feedback on how they compare in terms of performance, pricing, and overall experience.\\n\\nHas anyone used GPUs from either of these providers? How do they stack up against each other in terms of power efficiency, speed, and reliability? Are there any specific use cases where one outperforms the other?\\n\\nI’d love to hear your thoughts, personal experiences, or any suggestions you might have on which GPU might be better for intensive workloads. Thanks in advance!'}\n",
      "{'id': '1i0c3x8', 'title': 'Humana Senior DS Position merry-go-round', 'author': 'tinkinc', 'subreddit': 'datascience', 'upvotes': 24, 'created_utc': 1736768520.0, 'text': 'Anyone in the US apply to the Humana revolving Senior DS position over the last 5 months? They continuously post this position and never seem to fill it. Wondering if anyone has gotten an actual interview. I make it to the prescreen rounds  every single time I apply and then it just gets reposted.  '}\n",
      "{'id': '1i0czn6', 'title': 'Sky-T1-32B: Open-sourced reasoning model outperforms OpenAI-o1 on coding and maths benchmarks ', 'author': 'mehul_gupta1997', 'subreddit': 'datascience', 'upvotes': 1, 'created_utc': 1736771828.0, 'text': ''}\n",
      "{'id': '1i0dbaj', 'title': 'Mastering The Poisson Distribution: Intuition and Foundations', 'author': 'chomoloc0', 'subreddit': 'datascience', 'upvotes': 146, 'created_utc': 1736772966.0, 'text': ''}\n",
      "{'id': '1i0m1ts', 'title': \"Advice on stabilizing an autoencoder's representation?\", 'author': 'empirical-sadboy', 'subreddit': 'datascience', 'upvotes': 3, 'created_utc': 1736795972.0, 'text': ''}\n",
      "{'id': '1i0wxxt', 'title': 'Mistral released Codestral 25.01 : Free to use with VS Code and Jet brains', 'author': 'mehul_gupta1997', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1736825403.0, 'text': ''}\n",
      "{'id': '1i0x2pm', 'title': 'Fuck pandas!!! [Rant]', 'author': 'SnooLobsters8778', 'subreddit': 'datascience', 'upvotes': 483, 'created_utc': 1736825814.0, 'text': 'I have been a heavy R user for 9 years and absolutely love R. I can write love letters about the R data.table package. It is fast. It is efficient. it is beautiful. A coder’s dream.\\n \\nBut of course all good things must come to an end and given the steady decline of R users decided to switch to python to keep myself relevant.\\n\\nAnd let me tell you I have never seen a stinking hot pile of mess than pandas. Everything is 10 layers of stupid? The syntax makes me scream!!!!!! There is no coherence or pattern ? Oh use [] here but no use ({}) here.\\nWant to do a if else ooops better download numpy. \\nWant to filter ooops use loc and then iloc and write 10 lines of code.\\n\\nIt is unfortunate there is no getting rid of this unintuitive maddening, mess of a library, given that every interviewer out there expects it!!! There are much better libraries and it is time the pandas reign ends!!!!! (Python data table even creates pandas data frame faster than pandas!)\\n\\nThank you for coming to my Ted talk\\nI leave you with this datatable comparison article while I sob about learning pandas \\n\\n'}\n",
      "{'id': '1i13e03', 'title': 'Seeking Advice on Amazon Bedrock and Azure', 'author': 'jameslee2295', 'subreddit': 'datascience', 'upvotes': 8, 'created_utc': 1736851459.0, 'text': 'Hello everyone. I’m currently exploring AI infrastructure and platform for a new project and I’m trying to decide between Amazon Bedrock and Azure (AI Infrastructure & AI Studio). I’ve been considering both but would love to hear about your real-world experiences with them.\\n\\nHas anyone used Amazon Bedrock or Azure AI Infrastructure and Azure AI Studio? How would you compare the two in terms of ease of use, performance, and overall flexibility? Are there specific features from either platform that stood out to you, or particular use cases where one was clearly better than the other?\\n\\nAny advice or insights would be greatly appreciated. Thanks in advance! '}\n",
      "{'id': '1i18xcv', 'title': 'Dash Python Incosistence Performance', 'author': 'OxheadGreg123', 'subreddit': 'datascience', 'upvotes': 5, 'created_utc': 1736869506.0, 'text': 'I\\'m currently working on a project using Dash Python. It was light and breezy in the beginning. I changed a few codes while maintaining the error at 0, test-running it once in a while just to check if the code change affected the website, and nothing bad happened. But after I left it for a few hours without changing anything, the website wouldn\\'t run anymore and showed me an \"Internal Server Error\". This happened way too many times, and it stresses me out, as I have to update most of the backend ASAP. Does anyone has any similar experience and manage to solve it? I\\'d like to know how.'}\n",
      "{'id': '1i1951j', 'title': 'exit cmd.exe from R (or python) without admin privilege', 'author': 'Due-Duty961', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1736870071.0, 'text': 'I run:\\n\\nsystem(\"TASKKILL /F /IM cmd.exe\")\\n\\nI get\\n\\nErreur�: le processus \"cmd.exe\" de PID 10333 n\\'a pas pu être arrêté.\\n\\nRaison�: Accès denied.\\n\\nErreur�: le processus \"cmd.exe\" de PID 11444 n\\'a pas pu être arrêté.\\n\\nRaison�: Accès denied.\\n\\n\\nI execute a batch file> a cmd open>a shiny open (I do my calculations)> a button on shiny should allow the cmd closing (and the shiny of course)\\n\\nI can close the cmd from command line but I get access denied when I try to execute it from R. Is there hope? I am on the pc company so I don\\'t have admin privilege'}\n",
      "{'id': '1i1bjhi', 'title': 'E-values: A modern alternative to p-values', 'author': 'Stochastic_berserker', 'subreddit': 'datascience', 'upvotes': 107, 'created_utc': 1736876152.0, 'text': \"In many modern applications - A/B testing, clinical trials, quality monitoring - we need to analyze data as it arrives. Traditional statistical tools weren't designed with this sequential analysis in mind, which has led to the development of new approaches.\\n\\nE-values are one such tool, specifically designed for sequential testing. They provide a natural way to measure evidence that accumulates over time. An e-value of 20 represents 20-to-1 evidence against your null hypothesis - a direct and intuitive interpretation. They're particularly useful when you need to:\\n\\n- Monitor results in real-time\\n- Add more samples to ongoing experiments\\n- Combine evidence from multiple analyses\\n- Make decisions based on continuous data streams\\n\\nWhile p-values remain valuable for fixed-sample scenarios, e-values offer complementary strengths for sequential analysis. They're increasingly used in tech companies for A/B testing and in clinical trials for interim analyses.\\n\\nIf you work with sequential data or continuous monitoring, e-values might be a useful addition to your statistical toolkit. Happy to discuss specific applications or mathematical details in the comments.\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\n\\nP.S: Above was summarized by an LLM.\\n\\nPaper: Hypothesis testing with e-values - https://arxiv.org/pdf/2410.23614\\n\\nCurrent code libraries:\\n\\nPython:\\n\\n- expectation: New library implementing e-values, sequential testing and confidence sequences (https://github.com/jakorostami/expectation)\\n\\n- confseq: Core library by Howard et al for confidence sequences and uniform bounds (https://github.com/gostevehoward/confseq)\\n\\n\\nR: \\n\\n- confseq: The original R implementation, same authors as above\\n\\n- safestats: Core library by one of the researchers in this field of Statistics, Alexander Ly. (https://cran.r-project.org/web/packages/safestats/readme/README.html)\\n\\n\"}\n",
      "{'id': '1i1wnxj', 'title': 'Leaving Public Sector for Private', 'author': 'Illustrious-Mind9435', 'subreddit': 'datascience', 'upvotes': 21, 'created_utc': 1736945403.0, 'text': \"Posting for a friend:\\n\\nCurrently in a an ostensibly manager level DS position in local government. They are in the final stages of interviewing for a Director level role at a private firm. Is the compensation change worth it (posted below) and are there any DS specific aspects they should consider? \\n\\nRight now they are an IC who occasionally manages, but it seems this new role might be 80-90% managing. Is that common for the private sector? I told them it doesn't seem worth it (I'm biased as I am also in the public sector), but they said the compensation combined with more interesting work might be worth it.\\n\\nPublic Sector:\\nManager\\n135k\\nPension (secure but only okay payout)\\nStudent Loan Forgiveness\\n\\nPrivate Sector:\\nDirector\\n165k\\n10-15% Bonus\\n401k 4% Match\\n\\n\"}\n",
      "{'id': '1i1z6pj', 'title': 'Who is the most hungry for AI / ML talent right now', 'author': 'pg860', 'subreddit': 'datascience', 'upvotes': 128, 'created_utc': 1736953054.0, 'text': 'I run a job search engine for Data Scientists. This week we added monitoring of the highest paid job openings in the last week. This is what I saw. It seems one company in particular wants to outbid everyone else. And this is not because of lack of competition - we monitor more than 30.000 companies including all of Fortune 100 and most of Fortune 1000. We index more than 60k data science jobs every month. \\n\\nSource: [jobs-in-data.com](https://jobs-in-data.com)\\n\\nhttps://preview.redd.it/sqxgf9u786de1.png?width=2438&format=png&auto=webp&s=476af7f8ec1456a3d3f0e27f2fea61d4519daa9c\\n\\n'}\n",
      "{'id': '1i20otn', 'title': 'aspirations of starting a data science consultancy ', 'author': 'AdFew4357', 'subreddit': 'datascience', 'upvotes': 36, 'created_utc': 1736957009.0, 'text': 'Has anyone ever here thought of how to use their skills to start their own consultancy or some kind of business? Lately ive been kinda feeling that it would be really nice to have something of my own to work one involving analytics. Working for a company is great experience, but part of me would really like to have a business that I own where I help small businesses who have data make sense of it with low hanging fruit solutions.\\n\\nJust a thought, but I’ve always thought of some sort of consultancy where clients are some sort of local business that collects data but doesn’t use it effectively or does not have the expertise on how to turn their data into insights that can be used. \\n\\nFor example, suppose you had three clients:\\n\\n1. Local gyms which have lots of membership data - my consultancy could offer services to measure engagement, etc and use demographic information to further understand gym goers - don’t know what “action” they could take but a thought \\n\\n2. Local shop has expenses they track and right now it’s all over the place. A dashboard that can help them view everything in one place\\n\\nSomething where, it’s tasks which are trivial for the average data scientist, but generate a lot of value for local businesses.\\n\\nBut maybe you can go deeper? I’m not sure how genAI works and haven’t played around with like any of these tools, but I’ve thought of ways these can be incorporated too.\\n\\nIdk, I just find working in the industry sole draining and I just want to be able to have something that I can call my own, work on my own schedule, and it lead to a lot more revenue than working for a company. \\n\\nIf anyone has any thoughts on what they have done, or how they have tried to do something, please let me know. Ideally I’d try and start this after 3-4 years of experience where I’ve built some niche industry experience. '}\n",
      "{'id': '1i275yh', 'title': 'WASM-powered codespaces for Python notebooks on GitHub', 'author': 'mmmmmmyles', 'subreddit': 'datascience', 'upvotes': 11, 'created_utc': 1736973521.0, 'text': 'During a hackweek, we built this project that allows you to run [marimo](https://github.com/marimo-team/marimo) and Jupyter notebooks directly from GitHub in a Wasm-powered, codespace-like environment. What makes this powerful is that we mount the GitHub repository\\'s contents as a filesystem in the notebook, making it really easy to share notebooks with data.\\n\\n**All you need to do is prepend** [`https://marimo.app`](https://marimo.app) **to any Python notebook on GitHub.** Some examples:\\n\\n* Jupyter Notebook:\\xa0[https://marimo.app/github.com/jakevdp/PythonDataScienceHandb...](https://marimo.app/github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.08-Sorting.ipynb)\\n* marimo notebook:\\xa0[https://marimo.app/github.com/marimo-team/marimo/blob/07e8d1...](https://marimo.app/github.com/marimo-team/marimo/blob/07e8d14109f7312f19916fd13e4046a561a740f8/examples/third_party/polars/polars_example.py)\\n\\nJupyter notebooks are automatically converted into marimo notebooks using basic static analysis and source code transformations. Our conversion logic assumes the notebook was meant to be run top-down, which is usually but not always true \\\\[2\\\\]. It can convert many notebooks, but there are still some edge cases.\\n\\nWe implemented the filesystem mount using our own FUSE-like adapter that links the GitHub repository’s contents to the Python filesystem, leveraging Emscripten’s filesystem API. The file tree is loaded on startup to avoid waterfall requests when reading many directories deep, but loading the file contents is lazy. For example, when you write Python that looks like\\n\\n    with open(\"./data/cars.csv\") as f:\\n        print(f.read())\\n    \\n    # or\\n    \\n    import pandas as pd\\n    pd.read_csv(\"./data/cars.csv\")\\n\\nbehind the scenes, you make a request \\\\[3\\\\] to\\xa0*https://raw.githubusercontent.com/<org>/<repo>/main/data/cars.csv*\\n\\nDocs:\\xa0[https://docs.marimo.io/guides/publishing/playground/#open-notebooks-hosted-on-github](https://docs.marimo.io/guides/publishing/playground/#open-notebooks-hosted-on-github)\\n\\n\\\\[2\\\\]\\xa0[https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/](https://blog.jetbrains.com/datalore/2020/12/17/we-downloaded-10-000-000-jupyter-notebooks-from-github-this-is-what-we-learned/)\\n\\n\\\\[3\\\\] We technically proxy it through the playground\\xa0[https://marimo.app](https://marimo.app)\\xa0to fix CORS issues and GitHub rate-limiting.\\n\\n**Why is this useful?**\\n\\nVieiwng notebooks on GitHub pages is limiting. They don\\'t allow external css or scripts so charts and advanced widgets can fail. They also aren\\'t itneractive so you can\\'t tweek a value or pan/zoom a chart. It is also difficult to share your notebook with code - you either need to host it somehwere or embed it inside your notebook. Just append `https://marimo.app/<github_url>`'}\n",
      "{'id': '1i28x7i', 'title': 'What do you think about building the pipeline first with bad models to start refining quickly?', 'author': 'imberttt', 'subreddit': 'datascience', 'upvotes': 40, 'created_utc': 1736978111.0, 'text': \"we have to build a computer vision application, I detect 4 main problems, \\n\\n\\n\\nget the highest quality training set, it is requiring lots of code and it may require lots of manual work to generate the ground truth\\n\\ntrain a classification model, two main orthogonal approaches are being considered and will be tested\\n\\ntrain a segmentation model\\n\\nconnect the dots and build the end to end pipeline\\n\\n  \\none teammate is working in the highest quality training set, and three other teammates in the classification models. I think it would be incredibly beneficial to have the pipeline as soon as possible integrated with the extremely simple models, and then iterate taking into account error metrics, as it gives us goals and this lets them test their module/section of the work also taking into account variation of the final metrics.\\n\\n  \\nthis would also help the other teams that depend on our output, web development can use a model, it is just a bad model, but we'll improve the results, the deployment work could also start now.\\n\\n  \\nwhat do you guys think about this approach? for me it looks like its all benefits and zero problems but I see some teammates are reluctant on building something that definitely fails at the beginning and I'm not definitely the most experienced data scientist.\"}\n",
      "{'id': '1i29a6d', 'title': 'Advanced Imputation Techniques for Correlated Time Series: Insights and Experiences?\\n\\n', 'author': 'Super-Silver5548', 'subreddit': 'datascience', 'upvotes': 8, 'created_utc': 1736979031.0, 'text': \"**Advanced Imputation Techniques for Correlated Time Series: Insights and Experiences?**\\n\\nHi everyone,\\n\\nI’m looking to spark a discussion about **advanced imputation techniques** for datasets with multiple distinct but correlated time series. Imagine a dataset like **energy consumption** or **sales data**, where hundreds of stores or buildings are measured separately. The granularity might be hourly or daily, with varying levels of data completeness across the time series.\\n\\nHere’s the challenge:\\n\\n1. Some buildings/stores have **complete or nearly complete data** with only a few missing values. These are straightforward to impute using standard techniques.\\n2. Others have **partial data**, with gaps ranging from days to months.\\n3. Finally, there are buildings with **100% missing values** for the target variable across the entire time frame, leaving us reliant on correlated data and features.\\n\\nThe time series show **clear seasonal patterns** (weekly, annual) and dependencies on external factors like weather, customer counts, or building size. While these features are available for all buildings—including those with no target data—the features alone are insufficient to accurately predict the target. Correlations between the time series range from moderate (\\\\~0.3) to very high (\\\\~0.9), making the data situation highly heterogeneous.\\n\\n# My Current Approach:\\n\\nFor stores/buildings with **few or no data points**, I’m considering an approach that involves:\\n\\n1. **Using Correlated Stores**: Identify stores with high correlations based on available data (e.g., monthly aggregates). These could serve as a foundation for imputing the missing time series.\\n2. **Reconciling to Monthly Totals**: If we know the **monthly sums** of the target for stores with missing hourly/daily data, we could constrain the imputed time series to match these totals. For example, adjust the imputed hourly/daily values so that their sum equals the known monthly figure.\\n3. **Incorporating Known Features**: For stores with missing target data, use additional features (e.g., holidays, temperature, building size, or operational hours) to refine the imputed time series. For example, if a store was closed on a Monday due to repairs or a holiday, the imputation should respect this and redistribute values accordingly.\\n\\n# Why Just Using Correlated Stores Isn’t Enough:\\n\\nWhile using highly correlated stores for imputation seems like a natural approach, it has limitations. For instance:\\n\\n* A store might be closed on certain days (e.g., repairs or holidays), resulting in zero or drastically reduced energy consumption. Simply copying or scaling values from correlated stores won’t account for this.\\n* The known features for the missing store (e.g., building size, operational hours, or customer counts) might differ significantly from those of the correlated stores, leading to biased imputations.\\n* Seasonal patterns (e.g., weekends vs. weekdays) may vary slightly between stores due to operational differences.\\n\\n# Open Questions:\\n\\n* **Feature Integration**: How can we better incorporate the available features of stores with 100% missing values into the imputation process while respecting known totals (e.g., monthly sums)?\\n* **Handling Correlation-Based Imputation**: Are there specific techniques or algorithms that work well for leveraging correlations between time series for imputation?\\n* **Practical Adjustments**: When reconciling imputed values to match known totals, what methods work best for redistributing values while preserving the overall seasonal and temporal patterns?\\n\\nFrom my perspective, this approach seems sensible, but I’m curious about others' experiences with similar problems or opinions on why this might—or might not—work in practice. If you’ve dealt with imputation in datasets with heterogeneous time series and varying levels of completeness, I’d love to hear your insights!\\n\\nThanks in advance for your thoughts and ideas!\\n\\n\"}\n",
      "{'id': '1i2jytl', 'title': 'Start freelancing with 0 experience?', 'author': 'tropianhs', 'subreddit': 'datascience', 'upvotes': 49, 'created_utc': 1737013947.0, 'text': \"I hear many people have the ambition to start freelancing as soon as they can, ideally before having significant job experience. \\nI like the attitude, but I tried myself a few years ago and got burned. So I wanna share my experience. \\n   \\nI am a Data Scientist and tried to start freelancing with just one year job experience in 2017. Did the usual stuff. Set up an Upwork profile, applied to jobs at nights and during weekends and waited for a reply. \\nCrickets. I **applied to 11 jobs** and didn't get any. Looking back at that experience I see a few mistakes\\n1 I didn't have a portfolio of projects that matched the jobs I applied to. \\n2 I only used Upwork, without leveraging LInkedIn, Catalant, Fiverr and others. \\n3 I gave up too early. Just 11 applications over one month is not enough. I recommend applying to 20-30 jobs per week if possible.\\n4 I set an unreasonable hourly rate. I set my hourly rate same as my daily job, Freelancing is a market where you are the product. When there is no demand for you (because nobody knows you) it's a smart move to set the price low. Once demand picks up, increase the price accordingly. \\n\\nOverall, I think experience is not the number one factor that a client looks for when hiring a freelancer. It's way more important to give the client confidence that you can do the job. So you should always work with that goal in mind, from the way you build your profile, to all the communication with your client. \\nLast bit of advice. I found success in my local market at first. In Italy there is not many Data professionals that are also freelancers, and that helped me. People like to work with familiar faces and speaking the same language, sharing the same culture, goes a long way building confidence.\\n\\nCurious to know your point of view too. \"}\n",
      "{'id': '1i2m3mv', 'title': 'What Challenges Do Businesses Face When Developing AI Solutions?', 'author': 'jameslee2295', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1737023704.0, 'text': 'Hello everyone,\\n\\nI’m currently working on providing cloud services and looking to better understand the challenges businesses face when developing AI. As a cloud provider, I’m keen to learn about the real-world obstacles organizations encounter when scaling their AI solutions.\\n\\nFor those in the AI industry, what specific issues or limitations have you faced in terms of infrastructure, platform flexibility, or integration challenges? Are there any key challenges in AI development that remain unresolved? What specific support or solutions do AI developers need from cloud providers to overcome current limitations?\\n\\nLooking forward to hearing your thoughts and learning from your experiences. Thanks in advance!'}\n",
      "{'id': '1i2mh17', 'title': 'Solution completeness and take home assignments for interviews?', 'author': 'Tarneks', 'subreddit': 'datascience', 'upvotes': 6, 'created_utc': 1737025307.0, 'text': 'What is the general consensus about take home interviews and then completeness of solution.\\n\\nI have around a week and it took me already 2 days just to work with with the data just so I can\\n1) clean it\\n2) enhance it with external data\\n3) feature engineer it\\n4) establish baselines to capture lift\\n\\nThe whole thing is supposed to be finished around the span of a week. As i was scoping it out the whole thing is essentially potentially 3-4 models in a framework given the complex nature of the work.\\n\\nHow critical is the completeness and assumptions being made regarding these take home assignments. I didnt get a take home that large in scope. Its difficult task but very doable just laborious in the sense that it requires to be well thought out. '}\n",
      "{'id': '1i2qj4j', 'title': 'Books on Machine Learning + in R', 'author': 'geotheory', 'subreddit': 'datascience', 'upvotes': 29, 'created_utc': 1737038919.0, 'text': 'I\\'m interested in everyone\\'s experience of books based specifically in R on machine learning, deep learning, and more recently LLM modelling, etc.  If you have particular experience to share it would really useful to hear about it.\\n\\nAs a sub-question it would be great to hear about books intended for relative beginners, by which I mean those familiar with R and statistical analysis but with no formal training in AI. There is obviously the well-known *\"Introduction to Machine Learning with R\"* by Scott V Burger, available as a [free pdf](https://edisciplinas.usp.br/pluginfile.php/8527271/mod_resource/content/0/Burger%2C%20Scott%20V%20-%20Introduction%20to%20machine%20learning%20with%20R_%20rigorous%20mathematical%20analysis-OReilly%20%282018%29.pdf).  But it hasn\\'t been updated in nearly 7 years now, and a quick [scan of Google](https://www.google.co.uk/search?tbm=shop&hl=en-GB&psb=1&ved=2ahUKEwi6jeS9vPqKAxXdQ0ECHRvSDwAQu-kFegQIABAK&q=Machine+Learning+in+R&oq=Machine+Learning+in+R&gs_lp=Egtwcm9kdWN0cy1jYyIVTWFjaGluZSBMZWFybmluZyBpbiBSSABQAFgAcAB4AJABAJgBAKABAKoBALgBA8gBAJgCAKACAJgDAJIHAKAHAA&sclient=products-cc) shows quite a number of others.  Suggestions much appreciated.'}\n",
      "{'id': '1i2vj0x', 'title': 'Free Learning Paths for Data Analysts, Data Scientists, and Data Engineers – Using 100% Open Resources ', 'author': 'Ryan_3555', 'subreddit': 'datascience', 'upvotes': 263, 'created_utc': 1737051786.0, 'text': 'Hey, I’m Ryan, and I’ve created \\n\\nhttps://www.datasciencehive.com/learning-paths \\n\\na platform offering free, structured learning paths for data enthusiasts and professionals alike.\\n\\nThe current paths cover:\\n\\n\\t•\\tData Analyst: Learn essential skills like SQL, data visualization, and predictive modeling.\\n\\t•\\tData Scientist: Master Python, machine learning, and real-world model deployment.\\n\\t•\\tData Engineer: Dive into cloud platforms, big data frameworks, and pipeline design.\\n\\nThe learning paths use 100% free open resources and don’t require sign-up. Each path includes practical skills and a capstone project to showcase your learning.\\n\\nI see this as a work in progress and want to grow it based on community feedback. Suggestions for content, resources, or structure would be incredibly helpful.\\n\\nI’ve also launched a Discord community (https://discord.gg/Z3wVwMtGrw) with over 150 members where you can:\\n\\n\\t•\\tCollaborate on data projects\\n\\t•\\tShare ideas and resources\\n\\t•\\tJoin future live hangouts for project work or Q&A sessions\\n\\nIf you’re interested, check out the site or join the Discord to help shape this platform into something truly valuable for the data community.\\n\\nLet’s build something great together.\\n\\nWebsite: https://www.datasciencehive.com/learning-paths\\nDiscord: https://discord.gg/Z3wVwMtGrw '}\n",
      "{'id': '1i2vmuv', 'title': 'Introducing mlsynth.', 'author': 'turingincarnate', 'subreddit': 'datascience', 'upvotes': 22, 'created_utc': 1737052050.0, 'text': 'Hi DS Reddit. For those of who you work in causal inference, you may be interested in a Python library I developed called \"machine learning synthetic control\", or \"mlsynth\" for short.\\n\\nAs I write in its [documentation](https://mlsynth.readthedocs.io), mlsynth is a one-stop shop of sorts for implementing some of the most recent synthetic control based estimators, many of which use machine learning methodologies. Currently, the software is hosted from my GitHub, and it is still undergoing developments (i.e., for computing inference for point-estinates/user friendliness).\\n\\nmlsynth implements the following methods: [Augmented Difference-in-Differences](https://doi.org/10.1287/mksc.2022.1406), CLUSTERSCM, [Debiased Convex Regression](https://doi.org/10.1287/inte.2023.0028)  (undocumented at present), the [Factor Model Approach](https://doi.org/10.1177/00222437221137533), [Forward Difference-in-Differences](https://doi.org/10.1287/mksc.2022.1406), [Forward Selected Panel Data Approach](https://doi.org/10.1016/j.jeconom.2021.04.009), the [L1PDA](https://doi.org/10.1002/jae.1230), the [L2-relaxation PDA](https://doi.org/10.13140/RG.2.2.11670.97609), [Principal Component Regression](https://doi.org/10.1080/01621459.2021.1928513), [Robust PCA Synthetic Control](https://academicworks.cuny.edu/gc_etds/4984), [Synthetic Control Method (Vanilla SCM)](https://doi.org/10.1198/jasa.2009.ap08746), [Two Step Synthetic Control](https://doi.org/10.1287/mnsc.2023.4878)  and finally the two newest methods which are not yet fully documented, [Proximal Inference-SCM](https://arxiv.org/abs/2108.13935) and [Proximal Inference with Surrogates-SCM](https://arxiv.org/abs/2308.09527)  \\n\\nWhile each method has their own options (e.g., Bayesian or not, l2 relaxer versus L1), all methods have a common syntax which allows us to switch seamlessly between methods without needing to switch softwares or learn a new syntax for a different library/command. It also brings forth methods which either had no public documentation yet, or were written mostly for/in MATLAB.\\n\\nThe documentation that currently exists explains installation as well as the basic methodology of each method. I also provide worked examples from the academic literature to serve as a reference point for how one may use the code to estimate causal effects.\\n\\nSo, to anybody who uses Python and causal methods on a regular basis, this is an option that may suit your needs better than standard techniques.'}\n",
      "{'id': '1i33mt0', 'title': \"I've been given the choice between being a Data Scientist or an Analytics Manager. Which would you choose and why?\", 'author': 'UnsafeBaton1041', 'subreddit': 'datascience', 'upvotes': 198, 'created_utc': 1737073226.0, 'text': \"I'm coming from a Data Analyst position, and I've essentially been given the choice between being a Data Scientist and or an Analytics Manager. I thought Data Scientist was my dream job, but the Manager position would pay more, and I've been dreaming about working my way up to Director or CDO... Does Analytics Manager make the most sense in this case?\\n\\nUpdate for context: I'm 25, have a master's in data analytics, and have been working in the same industry for 7 years but in different roles. I've been an Analyst for 1.5+ years, and previously was a Data Manager, and a Researcher.\"}\n",
      "{'id': '1i34tao', 'title': 'looking for arts sales data to understand arts pricing dynamics or madness', 'author': 'Think_Huckleberry299', 'subreddit': 'datascience', 'upvotes': 1, 'created_utc': 1737076708.0, 'text': \"I would like to explore datasets of arts sale and auctions, please if anyone has a good source please post below in the link. Just curious to explore if there are any patterns in art prices or just maddness which data science can't understand why a banana and tape would sell for 6 million or perhaps I can learn more about arts from this dataset. \\n\\nthanks in advance\\n\\n\\n\\nThanks \"}\n",
      "{'id': '1i3a227', 'title': 'Google Titans : New LLM architecture with better long term memory', 'author': 'mehul_gupta1997', 'subreddit': 'datascience', 'upvotes': 9, 'created_utc': 1737094288.0, 'text': ''}\n",
      "{'id': '1i3a45a', 'title': 'How long did it take you to get a new role when looking for a new job?', 'author': 'Relative_Practice_93', 'subreddit': 'datascience', 'upvotes': 50, 'created_utc': 1737094521.0, 'text': \"I'm feeling very miserable at my job as well as feeling uneasy with the ethics of my company so I desperately am looking for a new role, but this job market is concerning. I have a BS in Math and MS in DS, been at my job as a data scientist for 1.5 years, worked for 3 years between BS and MS in analyst roles. Is there hope to have something new soon? How many apps per day should I be sending? \"}\n",
      "{'id': '1i3bwdj', 'title': 'Can someone help me understand what is the issue exactly?', 'author': 'meis_xry', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1737102536.0, 'text': ''}\n",
      "{'id': '1i3cgo0', 'title': \"guys is web crawling and scraping +1 for data science or it doesn't matter. \", 'author': '1_plate_parcel', 'subreddit': 'datascience', 'upvotes': 41, 'created_utc': 1737105217.0, 'text': 'by web crawling and scraping i mean advanced scraping with multiple websites for prices and products then building further things around it like strategic planning and buisness analytics. \\n\\nedit: is it a necessary skill or not. +1 it means its a great add on to ur skill stack'}\n",
      "{'id': '1i3clrk', 'title': 'Microsoft MatterGen: GenAI model for Material design and discovery ', 'author': 'mehul_gupta1997', 'subreddit': 'datascience', 'upvotes': 2, 'created_utc': 1737105866.0, 'text': ''}\n",
      "{'id': '1i3x2cf', 'title': 'Are there any ways to earn a little extra money on the side as a data scientist?', 'author': 'jmhimara', 'subreddit': 'datascience', 'upvotes': 102, 'created_utc': 1737165883.0, 'text': \"Using data science skills (otherwise I'm sure there are plenty). \\n\\nI know there is data annotation, but I'm not sure that qualifies as data science.\"}\n",
      "{'id': '1i3y1qs', 'title': 'What salary range should I expect as a fresh college grad with a BS in Statistics and Data Science?', 'author': 'Voldemort57', 'subreddit': 'datascience', 'upvotes': 128, 'created_utc': 1737169081.0, 'text': 'For context, I’m a student at UCLA, and am applying to jobs within California. But I’m interested in people’s past jobs fresh out of college, where in the country, and what the salary was. \\n\\nTentatively, I’m expecting a salary of anywhere between $70k and $80k, but I’ve been told I should be expecting closer to $100k, which just seems ludicrous. '}\n",
      "{'id': '1i3zajz', 'title': \"Huggingface smolagents : Code centric Agent framework. Is it the best AI Agent framework? I don't think so\", 'author': 'mehul_gupta1997', 'subreddit': 'datascience', 'upvotes': 2, 'created_utc': 1737173285.0, 'text': ''}\n",
      "{'id': '1i40izz', 'title': 'Do these recruiters sound like a scam?', 'author': 'Tamalelulu', 'subreddit': 'datascience', 'upvotes': 14, 'created_utc': 1737177777.0, 'text': \"Hi all, unsure of where else to ask this so asking here. \\n\\nI had a recruiter (heavy Indian accent) call/email me with an interesting proposition. They work for the candidate rather than the company. If they place you in a job within 45 days they ask for 9% of your first year's salary.\\n\\nThey claim their value add is in a couple of things. First they promise that they have advanced ATS software that will help tweak professional qualifications. Second, they say they will apply to approximately 50 JDs per day (I am skeptical this many relevant jobs are even being posted).\\n\\nI have never had luck with Indian recruiters before but I have had good experiences professionally in offshoring some repetitive tasks for cheap. This process sounds like it fits the bill. The part where it gets sketchy is they want either access to my LinkedIn/Gmail or they want me to create second LinkedIn/Gmail accounts that they would have control over. Access to my gmail is a nonstarter obviously. But creating spoof LinkedIn/Gmails feels a little sketchy. \\n\\nIf we're living in a universe where these guys are simply trying to provide the service they've described, I'm all in. I just don't want to get soft-rolled into some sort of scam.\"}\n",
      "{'id': '1i4f1go', 'title': 'AI is difficult to get right: Apple Intelligence rolled back(Mostly the summary feature)', 'author': 'takuonline', 'subreddit': 'datascience', 'upvotes': 313, 'created_utc': 1737227935.0, 'text': \"Source: https://edition.cnn.com/2025/01/16/media/apple-ai-news-fake-headlines/index.html#:\\\\~:text=Apple%20is%20temporarily%20pulling%20its,organization%20and%20press%20freedom%20groups.\\n\\nSeems like even Apple is struggling to deploy AI and deliver real-world value.  \\nYes, companies can make mistakes, but Apple rarely does, and even so, it seems like most of Apple Intelligence is not very popular with IOS users and has led to the creation of r/AppleIntelligenceFail.\\n\\nIt's difficult to get right in contrast to application development which was the era before the ai boom. \"}\n",
      "{'id': '1i4yyoe', 'title': 'Influential Time-Series Forecasting Papers of 2023-2024: Part 1', 'author': 'nkafr', 'subreddit': 'datascience', 'upvotes': 187, 'created_utc': 1737294977.0, 'text': 'This article explores some of the latest advancements in time-series forecasting.\\n\\nYou can find the article [here](https://aihorizonforecast.substack.com/p/influential-time-series-forecasting).\\n\\nEdit: If you know of any other interesting papers, please share them in the comments.'}\n",
      "{'id': '1i5576y', 'title': 'Should I Try to postpone my FAANG Interview?', 'author': 'FellowZellow', 'subreddit': 'datascience', 'upvotes': 211, 'created_utc': 1737311263.0, 'text': \"So I got contacted by a FAANG Recruiter for a Data Scientist Role I applied for a month and a half ago. But as I have started to prep, I realize I am not ready and need 1 to 2 months before I would be able to do well on all the technical interviews (there are 4 of them). My SQL is rusty because I have been using Pyspark so much that I didn't really need to do medium to hard SQL queries at work (We're also not allowed in most cases since SQL is slower). So I would just do everything in Pyspark. But now, as I start practicing my SQL I realize it's very basic, and it's going to take some time before I can get it on the level my pyspark is at.\\n\\nI've noticed that I feel like there is no chance of me performing well enough on this interview, and it sucks because the recruiter said that the hiring manager was looking at my resume and really wants to interview me as soon as possible since he thinks I have strong experience for the role (They made me bypass the phone screens because of it). I have no doubt I would be able to do the role, but interviews are another beast. According to the prep guide, my Stats, ML Theory, SQL, and Python all have to be perfect. Since I joined my current company as an intern, I didn't have to do as many in-depth technicals as I have to do here. I've interviewed at a couple other big companies last year and didn't make it to the final round for one simply because I needed more time to prepare. The FAANG recruiter wants me to do the first 2 interviews within the next two weeks, and I'm worried about what it would do to my confidence if I failed this interview since this is pretty much my dream Data Scientist role. My mind is already telling me just to make the best of this and use it as a learning experience, but another part of me is wondering if I should just cancel it altogether or try to delay it as much as possible. I have a mock interview with a Company Data Scientist they set up for me in a few days, but part of me feels defeated already and it sucks...\\n\\nI honestly am not sure what to do as I need a lot more time. I've heard others say it took them as long as 2-6 months before they were ready to crush their FAANG interview and I know I am not there yet...\"}\n",
      "{'id': '1i57vx1', 'title': 'Where to Start when Data is Limited: A Guide', 'author': 'usernamehere93', 'subreddit': 'datascience', 'upvotes': 70, 'created_utc': 1737317875.0, 'text': '\\nHey, I’ve put together an article on my thoughts and some research around how to get the most out of small datasets when performance requirements mean conventional analysis isn’t enough. \\n\\nIt’s aimed at helping people get started with new projects who have already started with the more traditional statistical methods.\\n\\nWould love to hear some feedback and thoughts.'}\n",
      "{'id': '1i5d77u', 'title': 'Anyone ever feel like working as a data scientist at hinge?', 'author': 'AdFew4357', 'subreddit': 'datascience', 'upvotes': 443, 'created_utc': 1737331692.0, 'text': 'Need to figure out what that damn algorithm is doing to keep me from getting matches lol. On a serious note I have read about some interesting algorithmic work at dating app companies. Any data scientists here ever worked for a dating app company? \\n\\nEdit: gale-shapely algorithm\\n\\nhttps://reservations.substack.com/p/hinge-review-how-does-it-work#:~:text=It%20turns%20out%20that%20the,among%20those%20who%20prefer%20them.'}\n",
      "{'id': '1i5inrb', 'title': 'Weekly Entering & Transitioning - Thread 20 Jan, 2025 - 27 Jan, 2025', 'author': 'AutoModerator', 'subreddit': 'datascience', 'upvotes': 10, 'created_utc': 1737349304.0, 'text': \" \\n\\nWelcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\\n\\n* Learning resources (e.g. books, tutorials, videos)\\n* Traditional education (e.g. schools, degrees, electives)\\n* Alternative education (e.g. online courses, bootcamps)\\n* Job search questions (e.g. resumes, applying, career prospects)\\n* Elementary questions (e.g. where to start, what next)\\n\\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\"}\n",
      "{'id': '1i60m31', 'title': 'Question about Using Geographic Data for Soil Analysis and Erosion Studies', 'author': 'Proof_Wrap_2150', 'subreddit': 'datascience', 'upvotes': 11, 'created_utc': 1737406032.0, 'text': 'I’m working on a project involving a dataset of latitude and longitude points, and I’m curious about how these can be used to index or connect to meaningful data for soil analysis and erosion studies. Are there specific datasets, tools, or techniques that can help link these geographic coordinates to soil quality, erosion risk, or other environmental factors?\\n\\nI’m interested in learning about how farmers or agricultural researchers typically approach soil analysis and erosion management. Are there common practices, technologies, or methodologies they rely on that could provide insights into working with geographic data like this?\\n\\nIf anyone has experience in this field or recommendations on where to start, I’d appreciate your advice!'}\n",
      "{'id': '1i658fp', 'title': 'What should I do to build a strong foundation in developing?', 'author': 'VolunteerEdge56', 'subreddit': 'datascience', 'upvotes': 11, 'created_utc': 1737417755.0, 'text': 'I’m interested in becoming a developer. I’m currently proficient in Tableau, Alteryx, Power BI etc. \\n\\nI feel like there’s 1 million different avenues. I’m not sure which route to take. \\n\\nI want to get around a community, where I can connect and get exposed to more. I’m in the Miami area. \\n\\nI’ve checked out YouTube videos on Java script\\n\\nWhat do you all recommend? '}\n",
      "{'id': '1i6pu2t', 'title': 'How to get individual restaurant review data?', 'author': 'Guyserbun007', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1737485051.0, 'text': ''}\n",
      "{'id': '1i6qa6u', 'title': 'Analyzing changes to gravel height along a road', 'author': 'Proof_Wrap_2150', 'subreddit': 'datascience', 'upvotes': 5, 'created_utc': 1737486130.0, 'text': 'I’m working with a dataset that measures the height of gravel along a 50 km stretch of road at 10-meter intervals. I have two measurements:\\n\\nBaseline height: The original height of the gravel.\\n\\nNew height: A more recent measurement showing how the gravel has decreased over time.\\n\\nThis gives me the difference in height at various points along the road. I’d like to model this data to understand and predict gravel depletion. \\n\\nHere’s what I’m considering:Identifying trends or patterns in gravel loss (e.g., areas with more significant depletion).\\n\\nUsing interpolation to estimate gravel heights at points where measurements are missing.\\n\\nExploring possible environmental factors that could influence depletion (e.g., road curvature, slope, or proximity to towns).\\n\\nHowever, I’m not entirely sure how to approach this analysis. Some questions I have:\\n\\nWhat are the best methods to visualize and analyze this type of spatial data?\\n\\nAre there statistical or machine learning models particularly suited for this?\\n\\nIf I want to predict future gravel heights based on the current trend, what techniques should I look into? Any advice, suggestions, or resources would be greatly appreciated!'}\n",
      "{'id': '1i6qcch', 'title': 'Is this a normal data analyst experience? Expectations for new data analysts in the field', 'author': 'Born_Supermarket_330', 'subreddit': 'datascience', 'upvotes': 51, 'created_utc': 1737486274.0, 'text': \"I am a data analyst for a corporate company, this is my first year in a role like this and it has been a year. My manager is concerned that I have holes in my understanding about the company, but I feel like it is the lack of training and resources. I've never struggled so much in a role before, I previously worked in sales/sales admin for 5 years at a scientific company.\\n\\nWhen I was interviewed, I explained that I had no experience with pivot tables or vlookup. It was my understanding from the interview that they were looking for someone to mentor, and I was hired on for having a great attitude. During onboarding, I was given pretty surface level material to review and met maybe a handful of times with others on the teams on building basic reports. I've had to do a lot of studying on my own time. During the year though, I have continued to struggle on the reporting aspect of my job and feel the relationship strains at work because of it. I am proud to say that I have been practicing excel files online with sample data at home for months and can successfully create files on my own. I've asked to shadow and practice files at home, but I was told to just learn more about the company and ask more questions. This is the kind of scenario I keep running into at my current job:\\n\\nEx: A few weeks ago, I was tasked to create a report. I was told to look at a few automated reports and essentially play around/figure it out. I was trained on two automated reports, but had not been trained on the others. My team was a bit annoyed with my confusion on which report I should use and that I should know based on the data. They gave me a suggestion on what report to try. I played around with the data on my own and got like 70% with the data I had. I was told yesterday that they decided to pull data elsewhere (because it would cover everything they wanted on the report more easily)  from a space I don't have access to and haven't been trained on. \\n\\n\\n\\n\"}\n",
      "{'id': '1i6tsvc', 'title': 'Syracuse online MSDS', 'author': 'swootyswiggity69', 'subreddit': 'datascience', 'upvotes': 6, 'created_utc': 1737494772.0, 'text': '5 YoE DS here. Looking to get that next level piece of paper. Looking for something where I can complete a degree while doing full time job.\\n\\nAnybody have any experience? Cash grab program or similar to Georgia tech?\\n\\nThanks in advance! '}\n",
      "{'id': '1i7a8e0', 'title': 'DS interested in Lower level languages', 'author': 'NoteClassic', 'subreddit': 'datascience', 'upvotes': 11, 'created_utc': 1737549714.0, 'text': 'Hi community,\\n\\nI’m primarily DS with quite a number of years in DS and DE. I’ve mostly worked with on-site infrastructure. \\n\\nMy stack is currently Python, Julia, R… and my field of interest is numerical computing, OpenMP, MPI and GPU parallel computing (down the line)\\n\\nI’m curious as to how best to align my current work with high level languages with my interest in lower level languages. \\n\\nIf I were deciding based on work alone, Fortran will be the best language for me to learn as there’s a lot of legacy code we’d have to port in the next years. \\n\\nHowever, I’d like to develop in a language that’ll complement the skill set of a DS.\\n\\nMy current view is Julia, C and Fortran. However, I’m not completely sure of how useful these are outside of my very-specific field. \\n\\nAre there any other DS that have gone through this? How did you decide? What would you recommend? What factors did you consider.'}\n",
      "{'id': '1i7b8id', 'title': 'Graduated september 2024 and i am now looking for an entry level data engineering position , what do you think about my cv ?', 'author': 'GiovannaDio', 'subreddit': 'datascience', 'upvotes': 220, 'created_utc': 1737552944.0, 'text': ''}\n",
      "{'id': '1i7bnpd', 'title': 'Meta: Career Advice vs Data Science', 'author': 'meevis_kahuna', 'subreddit': 'datascience', 'upvotes': 153, 'created_utc': 1737554248.0, 'text': \"I joined the thread to learn about Data Science.  Something like 75 percent of the posts are peoples resumes and requests for career advice. I thought these were supposed to go into a weekly thread or something - I'm getting a warning about the weekly thread even as I'm posting this comment. \\n\\nCan anyone suggest alternative subs with more educational content?\"}\n",
      "{'id': '1i7eaj9', 'title': 'Scrapy MRO error without any references to conflicting packages', 'author': 'Tamalelulu', 'subreddit': 'datascience', 'upvotes': 1, 'created_utc': 1737561336.0, 'text': 'Hi all,\\n\\nI\\'m working on a little personal project, quantifying what technologies are most asked for in Data Science JDs. Really I\\'m more using it to work on my Python chops. I\\'m hitting a slightly perplexing error and I think ChatGPT has taken me as far as it possibly can on this one. \\n\\nWhen I attempt to crawl my spider I get this error:   \\nTypeError: Cannot create a consistent method resolution order (MRO) for bases Injectable, Generic\\n\\nPreviously the code was attempting to import Injectable from scrap\\\\_poet until I eventually inspected the package and saw that Injectable doesn\\'t exist. So I attempted to avoid using that entirely and omitted all references to Injectable in my code. Yet I\\'m still getting this error. Any thoughts?\\n\\nHere\\'s what the spider looks like: \\n\\n    import scrapy\\n    import csv\\n    from scrapy_autoextract import request_raw\\n    \\n    class JobSpider(scrapy.Spider):\\n        name = \"job_spider\"\\n        custom_settings = {\\n            \"DOWNLOADER_MIDDLEWARES\": {\\n                \"scrapy_autoextract.AutoExtractMiddleware\": 543,\\n            },\\n        }\\n    \\n        # Read URLs from links.csv and start requests\\n        def start_requests(self):\\n            with open(\"/adzuna_links.csv\", \"r\") as file:\\n                reader = csv.reader(file)\\n                for row in reader:\\n                    url = row[0] \\n                    yield request_raw(url=url, page_type=\"jobposting\", callback=self.parse)\\n    \\n        def parse(self, response):\\n            try:\\n                # Extract job details directly from the response JSON data returned by AutoExtract\\n                job_data = response.json().get(\"job_posting\", {})\\n    \\n                if job_data:\\n                    yield {\\n                        \"title\": job_data.get(\"title\"),\\n                        \"description\": job_data.get(\"description\"),\\n                        \"company\": job_data.get(\"hiringOrganization\", {}).get(\"name\"),\\n                        \"location\": job_data.get(\"jobLocation\", {}).get(\"address\"),\\n                        \"datePosted\": job_data.get(\"datePosted\"),\\n                    }\\n                else:\\n                    self.logger.error(f\"No job data extracted from {response.url}\")\\n    \\n            except Exception as e:\\n                self.logger.error(f\"Error parsing job data from {response.url}: {e}\")'}\n",
      "{'id': '1i86l83', 'title': 'Call for input: Regression discontinuity design, and interrupted time series', 'author': 'chomoloc0', 'subreddit': 'datascience', 'upvotes': 3, 'created_utc': 1737647935.0, 'text': ''}\n",
      "{'id': '1i894sd', 'title': 'Deep Learning in AdTech, a hands-on example with Kaggle', 'author': 'bweber', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1737654262.0, 'text': ''}\n",
      "{'id': '1i89r5d', 'title': 'The most in demand DS skills via 901 Adzuna listings', 'author': 'Tamalelulu', 'subreddit': 'datascience', 'upvotes': 694, 'created_utc': 1737655783.0, 'text': ''}\n",
      "{'id': '1i8czdb', 'title': 'I feel left behind on AWS or any cloud services overall', 'author': 'Careful_Engineer_700', 'subreddit': 'datascience', 'upvotes': 140, 'created_utc': 1737663750.0, 'text': 'Hi, I got promoted to a data scientist at work, from operations analysis to doing optimization and dynamic pricing, however, I only do code, good and clean one. But I feel like an analyst again but this time, on steroids! \\nThe only thing I touch is sagemaker jupyter lab to open my machine, and some s3 concepts, how to read write ther, nothing fancy. \\n\\nBut really that\\'s it, I only do deep analysis and that\\'s about it, there are people around me who do ML, deploy stuff, manage versions on GitHub, and so on... Doing stuff that is required from the market, when I tried applying out in other jobs, I really stood out for my analytical skills and math, statistics knowledge. But I REALLY lack practice! \\n\\nI know ML concepts, but I feel really rusty that I NEVER get to use it, except for linear regression and decision trees as I use them a lot in analysis. \\n\\nI got stuck in an interview when asked about redshift, eventbridge, other AWS services.\\n\\nMy teammates are super friendly, they are my age and we are good friends, When I talked to them, asked them to involve me in their projects, I just couldn\\'t have the time for it as their projects always conflicts with mine. They always tell me that \"you\\'ll know how to use them when you need them\", but I am afraid given my role condition, I will never get to use them, I analyze and stuff.\\n\\nWhat can I do guys, I could really use some advice, I don\\'t feel like I am doing fine, I feel left out. \\n\\nThanks. '}\n",
      "{'id': '1i8hb9n', 'title': 'Where is the standard ML/DL? Are we all shifting to prompting ChatGPT?', 'author': 'Franzese', 'subreddit': 'datascience', 'upvotes': 238, 'created_utc': 1737674877.0, 'text': 'I am working at a consulting company and while so far all the focus has been on cool projects involving setting up ML\\\\DL models, lately all the focus has been shifted on GenAI. As a data scientist/maching learning engineer who tackled difficult problems of data and modles, for the past 3 months I have been editing the same prompt file, saying things differently to make ChatGPT understand me. Is this the new reality? or should I change my environment? Please tell me there are standard ML projects.'}\n",
      "{'id': '1i8nauq', 'title': 'I made a guide to help people understand Docker', 'author': 'M0shka', 'subreddit': 'datascience', 'upvotes': 377, 'created_utc': 1737692941.0, 'text': 'When I first started out using Docker it was really confusing. I made a guide to help people understand what Docker is used for. Please let me know what you think and if you have any feedback \\n\\nhttps://youtu.be/QtH-RqFcDFc?si=PtQe7z7kZ2vlF_3Q'}\n",
      "{'id': '1i8t59p', 'title': 'Building a Reliable Text-to-SQL Pipeline: A Step-by-Step Guide pt.1', 'author': 'phicreative1997', 'subreddit': 'datascience', 'upvotes': 29, 'created_utc': 1737717457.0, 'text': ''}\n",
      "{'id': '1i90dzu', 'title': 'Imposter syndrome as a DS', 'author': 'None', 'subreddit': 'datascience', 'upvotes': 92, 'created_utc': 1737738624.0, 'text': 'Hello! I\\'m seeking some career advice and tips. I\\'ve essentially been pigeon-holed into a TPM position with a Data Scientist title for the past 2.5 years. This is my first official DS role, but I was in analytics for several years before. The team I joined had no real need for a data scientist, and have really been using me as a PM for reporting/partner management. I occasionally get to do data science \"projects\" but they let me decide what to analyze. Without real engagement from partners around business needs, this ends up being adhoc analyses with minimal business impact. I\\'ve been looking for a new role for over a year now but the market is terrible. I\\'m in the process of completing the OMSA program, so I\\'m not terribly rusty on stats/ML concepts, but I\\'m starting to feel insecure in my abilities to cut it as a DS IRL. A new hire recently joined a team within my broader org and asked me how I productionalize my code but I never have and it made me feel like an imposter. Does anyone have tips or encouragement? '}\n",
      "{'id': '1i90imp', 'title': 'DML researchers want to help me out here?', 'author': 'AdFew4357', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1737738942.0, 'text': '\\n\\nHey guys, I’m a MS statistician by background who has been doing my masters thesis in DML for about 6 months now. \\n\\nOne of the things that I have a question about is, does the functional form of the propensity and outcome model really not matter that much? \\n\\nMy advisor isn’t trained in this either, but we have just been exploring by fitting different models to the propensity and outcome model. \\n\\nWhat we have noticed is no matter you use xgboost, lasso, or random forests, the ATE estimate is damn close to the truth most of the time, and any bias is like not that much.\\n\\nSo I hate to say that my work thus far feels anti-climactic, but it feels kinda weird to done all this work to then just realize, ah well it seems the type of ML model doesn’t really impact the results.\\n\\nIn statistics I have been trained to just think about the functional form of the model and how it impacts predictive accuracy. \\n\\nBut what I’m finding is in the case of causality, none of that even matters.\\n\\n\\nI guess I’m kinda wondering if I’m on the right track here \\n\\n\\nEdit: DML = double machine learning '}\n",
      "{'id': '1i98tom', 'title': 'Data Imbalance Monitoring Metrics?', 'author': 'Emuthusiast', 'subreddit': 'datascience', 'upvotes': 5, 'created_utc': 1737760171.0, 'text': 'Hello all,\\n\\nI am consulting a business problem from a colleague with a dataset that has 0.3% of the class of interest.  The dataset  70k+ has observations, and we were debating on what  thresholds were selected for metrics robust to data imbalance , like PRAUC, Brier, and maybe MCC.\\n\\nDo you have any thoughts from your domains on how to deal with data imbalance problems and what performance metrics and thresholds to monitor them with ? As a an FYI, sampling was ruled out due to leading to models in need of strong calibration. Thank you all in advance.'}\n",
      "{'id': '1i9ar5b', 'title': 'What to expect from this Technical Test?', 'author': 'one_more_throwaway12', 'subreddit': 'datascience', 'upvotes': 51, 'created_utc': 1737765610.0, 'text': 'I applied for a SQL data analytics role and have a technical test with the following components\\n\\n* Multiple choice SQL questions (up to 10 mins)\\n* Multiple choice general data science questions (15 mins)\\n* SQL questions where you will write the code (20 mins)\\n\\nI can code well so Im not really worried about the coding part but do not know what to expect of the multiple choice ones as ive never had this experience before. I do not know much of the like infrastructure of sql of theory so dont know how to prepare, especially for the general data science questions which I have no idea what that could be. Any advice? '}\n",
      "{'id': '1i9gcxu', 'title': 'What GPU config to choose for AI usecases?', 'author': 'mehul_gupta1997', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1737784103.0, 'text': ''}\n",
      "{'id': '1i9n64r', 'title': 'Do you implement own high performance Python algorithms and in which language?', 'author': 'DataPastor', 'subreddit': 'datascience', 'upvotes': 54, 'created_utc': 1737812495.0, 'text': 'I want to implement some numerical algorithms as a Python library in a low level (compiled) language like C/Cython/Zig; C++/nanobind/pybind11; Rust/PyO3 – and want to listen to some experiences from this field. If you have some hands-on experience, which language and library have you used and what is your recommendation? I also have some experience with R/C++/Rcpp, but also want to learn to do this in Python.'}\n",
      "{'id': '1i9shbm', 'title': 'Seeking advice on organizing a sprawling Jupyter Notebook in VS Code', 'author': 'Proof_Wrap_2150', 'subreddit': 'datascience', 'upvotes': 117, 'created_utc': 1737827205.0, 'text': '\\nI’ve been using a single Jupyter Notebook for quite some time, and it’s evolved into a massive file that contains everything from data loading to final analysis. My typical process starts with importing data, cleaning it up, and saving the results for reuse in pickle files. When I revisit the notebook, I load these intermediate files and build on them with transformations, followed by exploratory analysis, visualizations, and insights.\\n\\nWhile this workflow gets the job done, it’s becoming increasingly chaotic. Some parts are clearly meant to be reusable steps, while others are just me testing ideas or exploring possibilities. It all lives in one place, which is convenient in some ways but a headache in others. I often wonder if there’s a better way to organize this while keeping the flexibility that makes Jupyter such a great tool for exploration.\\n\\nIf this were your project, how would you structure it? '}\n",
      "{'id': '1ia175l', 'title': '[Official] 2024 End of Year Salary Sharing thread', 'author': 'Omega037', 'subreddit': 'datascience', 'upvotes': 390, 'created_utc': 1737850714.0, 'text': 'This is the official thread for sharing your current salaries (or recent offers).\\n\\nSee\\xa0[last year\\'s Salary Sharing thread here](https://www.reddit.com/r/datascience/comments/18tevwk/official_2023_end_of_year_salary_sharing_thread/). There was also\\xa0[an unofficial one from an hour ago here](https://www.reddit.com/r/datascience/comments/1i9zcgm/unofficial_2024_salary_thread/).\\n\\nPlease only post salaries/offers if you\\'re including hard numbers, but feel free to use a throwaway account if you\\'re concerned about anonymity. You can also generalize some of your answers (e.g. \"Large biotech company\"), or add fields if you feel something is particularly relevant.\\n\\n**Title:**\\n\\n* **Tenure length:**\\n* **Location:**\\n   * **$Remote:**\\n* **Salary:**\\n* **Company/Industry:**\\n* **Education:**\\n* **Prior Experience:**\\n   * **$Internship**\\n   * **$Coop**\\n* **Relocation/Signing Bonus:**\\n* **Stock and/or recurring bonuses:**\\n* **Total comp:**\\n\\nNote that while the primary purpose of these threads is obviously to share compensation info, discussion is also encouraged.'}\n",
      "{'id': '1iadgz9', 'title': 'Why AI Agents will be a disaster', 'author': 'mehul_gupta1997', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1737895019.0, 'text': ''}\n",
      "{'id': '1iahg6i', 'title': 'Warantly period and coverage after resignation', 'author': 'furioncruz', 'subreddit': 'datascience', 'upvotes': 7, 'created_utc': 1737905585.0, 'text': 'I am leaving my current job. I have built tooling to automate ML processes, document everything, and transfer knowledge. Nevertheless, these systems are not battle-hardened yet, and those I am transferring to are either DevOps who know little ML or DS who have poor SWE skills. I suppose they would need my help later down the road. I already offered that I would be available for quick chats if they needed me. \\n\\nI was wondering what the norm is in handling these scenarios. Do people usually offer free consultation as a warranty, and for how long?'}\n",
      "{'id': '1iawkau', 'title': 'Free Product Analytics / Product Data Scientist Case Interview (with answers!)', 'author': 'productanalyst9', 'subreddit': 'datascience', 'upvotes': 187, 'created_utc': 1737942221.0, 'text': 'If you are interviewing for Product Analyst, Product Data Scientist, or Data Scientist Analytics roles at tech companies, you are probably aware that you will most likely be asked an analytics case interview question. It can be difficult to find real examples of these types of questions. I wrote an example of this type of question and included sample answers. Please note that you don’t have to get everything in the sample answers to pass the interview. If you would like to learn more about passing the Product Analytics Interviews, check out my\\xa0[blog post here](https://futureproductanalyst.substack.com/p/how-to-pass-the-product-analytics). If you want to learn more about passing the A/B test interview, check out\\xa0[this blog post](https://futureproductanalyst.substack.com/p/how-to-passing-the-ab-test-interview).\\n\\nIf you struggled with this case interview, I highly recommend these two books:\\xa0[Trustworthy Online Controlled Experiments](https://amzn.to/3EbBybQ)\\xa0and\\xa0[Ace the Data Science Interview](https://amzn.to/4jLGbd5)\\xa0(these are affiliate links, but I bought and used these books myself and vouch for their quality).\\n\\nWithout further ado, here is the sample case interview. If you found this helpful, please subscribe to [my blog](https://substack.com/@futureproductanalyst/p-148705045) because I plan to create more samples interview questions.\\n\\n\\\\_\\\\_\\\\_\\n\\n**Prompt:**\\xa0Customers who subscribe to Amazon Prime get free access to certain shows and movies. They can also buy or rent shows, as not all content is available for free to Prime customers. Additionally, they can pay to subscribe to channels such as Showtime, Starz or Paramount+, all accessible through their Amazon Prime account.\\n\\nIn case you are not familiar with Amazon Prime Video, the homepage typically has one large feature such as “Watch the Seahawks vs. the 49ers tomorrow!”. If you scroll past that, there are many rows of video content such as “Movies we think you’ll like”, “Trending Now”, and “Top Picks for You”. Assume that each row is either all free content, or all paid content. [Here is an example screenshot](https://imgur.com/a/PoWux79).\\n\\n# Question 1: What are the benefits to Amazon of focusing on optimizing what is shown to each user on the Prime Video home page?\\n\\nPotential answers:\\n\\n>!(looking for pros/cons, candidate should list at least 3 good answers)!<\\n\\n>!Showing the right content to the right customer on the Prime Video homepage has lots of potential benefits. It is important for Amazon to decide how to prioritize because the right prioritization could:!<\\n\\n* >!**Drive engagement:**\\xa0Highlighting free content ensures customers derive value from their Prime subscription.!<\\n* >!**Increase revenue:**\\xa0Promoting paid content or paid channels can drive additional purchases or subscriptions.!<\\n* >!**Customer satisfaction:**\\xa0Ensuring users find relevant and engaging content quickly leads to a better browsing experience.!<\\n* >!**Content discovery:**\\xa0Showcasing a mix of content encourages customers to explore beyond free offerings.!<\\n* >!**But keep in mind potential challenges:**\\xa0Overemphasis on paid content may alienate customers who want free content. They could think “I’m paying for Prime to get access to free content, why is Amazon pushing all this paid content”!<\\n\\n# Question 2: What key considerations should Amazon take into account when deciding how to prioritize content types on the Prime Video homepage?\\n\\nPotential answers:\\n\\n>!(Again the candidate should list at least 3 good answers)!<\\n\\n* >!**Free vs. paid balance:**\\xa0Ensure users see value in their Prime subscription while exposing them to paid options. This is a delicate balance - Amazon wants to upsell customers on paid content without increasing Prime subscription churn. Keep in mind that paid content is usually newer and more in demand (e.g. new releases)!<\\n* >!**User engagement:**\\xa0Consider the user’s watch history and preferences (e.g., genres, actors, shows vs. movies).!<\\n* >!**Revenue impact:**\\xa0Assess how prominently displaying paid content or channels influences rental, purchase, and subscription revenue.!<\\n* >!**Content availability:**\\xa0Prioritize content that is currently trending, newly released, or exclusive to Amazon Prime Video.!<\\n* >!**Geo and licensing restrictions:**\\xa0Adapt recommendations based on the content available in the user’s region.!<\\n\\n# Question 3: Let’s say you hypothesize that prioritizing free Prime content will increase user engagement. How would you measure whether this hypothesis is true?\\n\\nPotential answer:\\n\\n>!I would design an experiment where the treatment is that free Prime content is prioritized on row one of the homepage. The control group will see whatever the existing strategy is for row one (it would be fair for the candidate to ask what the existing strategy is. If asked, respond that the current strategy is to equally prioritize free and paid content in row one).!<\\n\\n>!To measure whether prioritizing free Prime content in row one would increase user engagement, I would use the following metrics:!<\\n\\n* >!**Primary metric:**\\xa0Average hours watched per user per week.!<\\n* >!**Secondary metrics:**\\xa0Click-through rate (CTR) on row one.!<\\n* >!**Guardrail metric:** Revenue from paid content and channels!<\\n\\n# Question 4: How would you design an A/B test to evaluate which prioritization strategy is most effective? Be detailed about the experiment design.\\n\\nPotential answer:\\n\\n>!1. Clearly State the Hypothesis:!<\\n\\n>!Prioritizing free Prime content on the homepage will increase engagement (e.g., hours watched) compared to equal prioritization of paid content and free content because free content is perceived as an immediate value of the Prime subscription, reducing friction of watching and encouraging users to explore and watch content without additional costs or decisions.!<\\n\\n>!2. Success Metrics:!<\\n\\n* >!Primary Metric:\\xa0Average hours watched per user per week.!<\\n* >!Secondary Metric:\\xa0Click-through rate (CTR) on row one.!<\\n\\n>!3. Guardrail Metrics:!<\\n\\n* >!Revenue from paid content and channels, per user:\\xa0Ensure prioritizing free content does not drastically reduce purchases or subscriptions.!<\\n   * >!Numerator: Total revenue generated from each experiment group from paid rentals, purchases, and channel subscriptions during the experiment.!<\\n   * >!Denominator: Total number of users in the experiment group.!<\\n* >!Bounce rate:\\xa0Ensure the experiment does not unintentionally make the homepage less engaging overall.!<\\n   * >!Numerator: Number of users who log in to Prime Video but leave without clicking on or interacting with any content.!<\\n   * >!Denominator: Total number of users who log in to Prime Video, per experiment group!<\\n* >!Churn rate:\\xa0Monitor for any long-term negative impact on overall customer retention.!<\\n   * >!Numerator: Number of Prime members who cancel their subscription during the experiment!<\\n   * >!Denominator: Total number of Prime members in the experiment.!<\\n\\n>!4. Tracking Metrics:!<\\n\\n* >!CTR on free, paid, and channel-specific recommendations. This will help us evaluate how well users respond to different types of content being highlighted.!<\\n   * >!Numerator: Number of clicks on free/paid/channel content cards on the homepage.!<\\n   * >!Denominator: Total number of impressions of free/paid/channel content cards on the homepage.!<\\n* >!Adoption rate of paid channels (percentage of users subscribing to a promoted channel).!<\\n\\n>!5. Randomization:!<\\n\\n* >!Randomization Unit:\\xa0Users (Prime subscribers).!<\\n* >!Why this will work:\\xa0User-level randomization ensures independent exposure to different homepage designs without contamination from other users.!<\\n* >!Point of Incorporation to the experiment:\\xa0Users are assigned to treatment (free content prioritized) or control (equal prioritization of free and paid content) upon logging in to Prime Video, or landing on the Prime Video homepage if they are already logged in.!<\\n* >!Randomization Strategy:\\xa0Assign users to treatment or control groups in a 50/50 split.!<\\n\\n>!6. Statistical Test to Analyze Metrics:!<\\n\\n* >!For continuous metrics (e.g., hours watched): t-test!<\\n* >!For proportions (e.g., CTR): Z-test of proportions!<\\n* >!Also, using regression is an appropriate answer, as long as they state what the dependent and independent variables are.!<\\n* >!Bonus points if candidate mentions CUPED for variance reduction, but not necessary!<\\n\\n>!7. Power Analysis:!<\\n\\n* >!Candidate should mention conducting a power analysis to estimate the required sample size and experiment duration. Don’t have to go too deep into this, but candidate should at least mention these key components of power analysis:!<\\n   * >!Alpha (e.g. 0.05), power (e.g. 0.8), MDE (minimum detectable effect) and how they would decide the MDE (e.g. prior experiments, discuss with stakeholders), and variance in the metrics!<\\n   * >!Do not have to discuss the formulas for calculating sample size!<\\n\\n# Question 5: Suppose the new prioritization strategy won the experiment, and is fully launched. Leadership wants a dashboard to monitor its performance. What metrics would you include in this dashboard?\\n\\nPotential answers:\\n\\n* >!**Engagement metrics:**!<\\n   * >!Average hours watched per user per week.!<\\n   * >!CTR on homepage recommendations (broken down by free, paid, and channel content).!<\\n   * >!CTR on by row!<\\n* >!**Revenue metrics:**!<\\n   * >!Revenue from paid content rentals and purchases.!<\\n   * >!Subscriptions to paid channels.!<\\n* >!**Retention metrics:**!<\\n   * >!Weekly active users (WAU).!<\\n   * >!Monthly active users (MAU).!<\\n   * >!Churn rate of Prime subscribers.!<\\n* >!**Operational metrics:**!<\\n   * >!Latency or errors in the recommendation algorithm.!<\\n   * >!User satisfaction scores (e.g., via feedback or surveys).!<'}\n",
      "{'id': '1ib0dfb', 'title': 'Weekly Entering & Transitioning - Thread 27 Jan, 2025 - 03 Feb, 2025', 'author': 'AutoModerator', 'subreddit': 'datascience', 'upvotes': 7, 'created_utc': 1737954098.0, 'text': \" \\n\\nWelcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\\n\\n* Learning resources (e.g. books, tutorials, videos)\\n* Traditional education (e.g. schools, degrees, electives)\\n* Alternative education (e.g. online courses, bootcamps)\\n* Job search questions (e.g. resumes, applying, career prospects)\\n* Elementary questions (e.g. where to start, what next)\\n\\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\"}\n",
      "{'id': '1ib33bq', 'title': 'Word of advice for job seekers', 'author': 'ResearchMindless6419', 'subreddit': 'datascience', 'upvotes': 255, 'created_utc': 1737964862.0, 'text': 'If your potential employer requires you to sign an NDA for a take home assignment, they’re exploiting you for free work. \\n\\nIn particular, if the work they want you to do is remarkably specific, definifely do not do it. \\n\\n'}\n",
      "{'id': '1ib8lg1', 'title': 'Sample size calculator with live data visualization as parameters change', 'author': 'vastava_viz', 'subreddit': 'datascience', 'upvotes': 24, 'created_utc': 1737983418.0, 'text': \"[Demo of live updating chart on samplesizecalc.com](https://i.redd.it/44ow6kyqcjfe1.gif)\\n\\nIt's been a while since I've worked on my sample size calculator tool ([last post here](https://www.reddit.com/r/datascience/comments/1e9f41u/easiest_way_to_calculate_required_sample_size_for/)). But I had a lot of fun adding an interactive chart to visualize required sample size, and thought you all would appreciate it! Made with d3.js\\n\\nCheck it out here: [https://www.samplesizecalc.com/calculator?metricType=proportion](https://www.samplesizecalc.com/calculator?metricType=proportion)\\n\\nWhat I love about this is that it helps me understand the relationship between each of the variables, statistical power and sample size. Hope it's a nice explainer for you all too.\\n\\nI also have plans to add a line chart to show how the statistical power increases over time (ie. the longer the experiment runs, the more samples you collect and the greater the power!)\\n\\nAs always, let me know if you run into any bugs.\"}\n",
      "{'id': '1ibedfy', 'title': 'as someone who aims to be a ML engineer, How much OOP and programming skills do i need ?', 'author': 'Emotional-Rhubarb725', 'subreddit': 'datascience', 'upvotes': 115, 'created_utc': 1737998200.0, 'text': 'When to stop on the developer track ? \\n\\nhow much do I need to master to help me being a good MLE \\n\\n'}\n",
      "{'id': '1ibkw2h', 'title': 'Would you rather be comfortable or take risks moving around?', 'author': 'JobIsAss', 'subreddit': 'datascience', 'upvotes': 21, 'created_utc': 1738013919.0, 'text': 'I recently received a job offer from a mid-to-large tech company in the gig economy space. The role comes with a competitive salary, offering a 15-20k increase over my current compensation. While the pay bump is nice, the job itself will be challenging as it focuses on logistics and pricing. However, I do have experience in pricing and have demonstrated my ability to handle optimization work. This role would also provide greater exposure to areas like causal inference, optimization, and real-time analytics, which are areas I’d like to grow in.\\n\\nThat said, I’m concerned about my career trajectory. I’ve moved around frequently in the past—for example, I spent 1.5 years at a big bank in my first role but left due to a toxic team. While I’m currently happy and comfortable in my role, I haven’t been here for a full year yet.\\n\\nMy current total compensation is $102k. While the work-life balance is great, my team is lacking in technical skills, and I’ve essentially been responsible for upskilling the entire practice. Another area of concern is that technically we are not able to keep up with bigger companies and the work is highly regulated so innovation isnt as easy.\\n\\nGiven the frequency move what would you do in my shoes? Take it and try to improve career opportunities for big tech?'}\n",
      "{'id': '1ibm1t4', 'title': 'Is there a way to terminate a running ML algorithm in python?', 'author': 'Guyserbun007', 'subreddit': 'datascience', 'upvotes': 12, 'created_utc': 1738016755.0, 'text': 'I have a set of ML algorithms to be fit to the same data on a df. Some of them takes days to run while others usually take minutes. What I\\'d like to do is to set up a max model fitting timer, so once the fitting/training of an algorithm exceeds that, it will forgot that algo and move onto the next one. Is there way to terminate the model.fit() after it is initiated based on a prespecified time? Here are my code excerpts.\\n\\n    ml_model_param_for_price_model_simple = {\\n                \\'Linear Regression\\': {\\n                    \\'model\\': LinearRegression(),\\n                    \\'params\\': {\\n                        \\'fit_intercept\\': [True, False],\\n                        \\'copy_X\\': [True, False],\\n                        \\'n_jobs\\': [None, -1]\\n                    }\\n                },\\n                \\'XGBoost Regressor\\': {\\n                    \\'model\\': XGBRegressor(objective=\\'reg:squarederror\\', random_state=random_state),\\n                    \\'params\\': {\\n                        \\'n_estimators\\': [100, 200, 300],\\n                        \\'learning_rate\\': [0.01, 0.1, 0.2],\\n                        \\'max_depth\\': [3, 5, 7],\\n                        \\'subsample\\': [0.7, 0.8, 1.0],\\n                        \\'colsample_bytree\\': [0.7, 0.8, 1.0]\\n                    }\\n                },\\n                \\'Lasso Regression\\': {\\n                    \\'model\\': Lasso(random_state=random_state),\\n                    \\'params\\': {\\n                        \\'alpha\\': [0.01, 0.1, 1.0, 10.0],  # Lasso regularization strength\\n                        \\'fit_intercept\\': [True, False],\\n                        \\'max_iter\\': [1000, 2000]  # Maximum number of iterations\\n                    }\\n                },        }\\n\\nThe looping and fitting of data below:\\n\\n    X = df[list_of_predictors]\\n    y = df[\\'outcome_var\\']\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=self.random_state)\\n    \\n    # Hyperparameter tuning and model training\\n    tuned_models = {}\\n    \\n    for model_name, current_param in self.param_grids.items():\\n        model = current_param[\\'model\\']\\n        params = current_param[\\'params\\']\\n    \\n        if params:  # Check if there are parameters to tune\\n            if model_name == \\'XGBoost Regressor\\':\\n                model = RandomizedSearchCV(\\n                    model, params, n_iter=10, cv=5, scoring=\\'r2\\', random_state=self.random_state\\n                )\\n            else:\\n                model = GridSearchCV(model, params, cv=5, scoring=\\'r2\\')\\n    \\n            start_time = datetime.now()  # Start timing\\n            model.fit(X_train, y_train) # NOTE: I want this to break out when a timer is done!!\\n            end_time = datetime.now()  # End timing\\n    \\n            tuned_models[model_name] = model.best_estimator_  # Store the best fitted model\\n            logger.info(f\"\\\\n{model_name} best estimator: {model.best_estimator_}\")\\n            logger.info(f\"{model_name} fitting time: {end_time - start_time}\")  # Print the fitting time\\n    \\n        else:\\n            start_time = datetime.now()  # Start timing\\n            model.fit(X_train, y_train)  # Fit model directly if no params to tune\\n            end_time = datetime.now()  # End timing\\n    \\n            tuned_models[model_name] = model  # Save the trained model\\n            logger.info(f\"{model_name} fitting time: {end_time - start_time}\")  # Print the fitting time'}\n",
      "{'id': '1iby5om', 'title': \"NVIDIA's paid Generative AI courses for FREE (limited period)\", 'author': 'mehul_gupta1997', 'subreddit': 'datascience', 'upvotes': 798, 'created_utc': 1738058049.0, 'text': 'NVIDIA has announced free access (for a limited time) to its premium courses, each typically valued between $30-$90, covering advanced topics in Generative AI and related areas.\\n\\nThe major courses made free for now are :\\n\\n* **Retrieval-Augmented Generation (RAG) for Production:** Learn how to deploy scalable RAG pipelines for enterprise applications.\\n* **Techniques to Improve RAG Systems:** Optimize RAG systems for practical, real-world use cases.\\n* **CUDA Programming:** Gain expertise in parallel computing for AI and machine learning applications.\\n* **Understanding Transformers:** Deepen your understanding of the architecture behind large language models.\\n* **Diffusion Models:** Explore generative models powering image synthesis and other applications.\\n* **LLM Deployment:** Learn how to scale and deploy large language models for production effectively.\\n\\n**Note:** There are redemption limits to these courses. A user can enroll into any one specific course.\\n\\n**Platform Link**: [NVIDIA TRAININGS](https://nvda.ws/4jtHiOf)'}\n",
      "{'id': '1ic0241', 'title': 'I hacked LLMs to work like scikit-learn', 'author': 'No_Information6299', 'subreddit': 'datascience', 'upvotes': 200, 'created_utc': 1738065934.0, 'text': 'A while ago I thought about using LLMs for classic machine learning tasks - which is stupid, I know? But I tried it anyway.\\n\\n**Never use it if:**\\n\\n* You have sufficient data and knowledge to train a specialized model\\n\\n**Do use it if:**\\n\\n* You need quick experimentation or you do not have enough data to train the model\\n\\n**Key findings:**\\n\\n|Dataset|IMDB 50k Dataset|Cats and dogs|\\n|:-|:-|:-|\\n|**Data**|Text data - Positive negative sentiment|Picture data - Predict what is on the picture|\\n|**Accuracy**|**96% -  SOTA (98+%)**|**97% -  SOTA (99%+)**|\\n|**Model**|gpt-4o-mini|gpt-4o-mini|\\n\\nAs you can see LLMs perform worse than SOTA specialized models, but if we have a use case with minimal data it can be very useful. \\n\\n# How can you play around?\\n\\nIt took some time to code it in a way that can be also used by others, here is a minimal example of how you can use it when applicable.\\n\\nYou can install FlashLearn using pip:\\n\\n    pip install flashlearn\\n\\n# Minimal Example - Classify Text\\n\\nBelow is a sample code snippet demonstrating how to classify text using FlashLearn in just 10 lines of code:\\n\\n    import os\\n    from openai import OpenAI\\n    from flashlearn.skills.classification import ClassificationSkill\\n    \\n    # You can use OpenAI or DeepSeek or any OpenAI compatible endpoint\\n    deep_seek = OpenAI(api_key=\\'YOUR DEEPSEEK API KEY\\', base_url=\"https://api.deepseek.com\")\\n    data = [{\"message\": \"Where is my refund?\"}, {\"message\": \"My product was damaged!\"}]\\n    skill = ClassificationSkill(\\n        model_name=\"gpt-4o-mini\",\\n        client=OpenAI(),\\n        categories=[\"billing\", \"product issue\"],\\n        system_prompt=\"Classify the request.\"\\n    )\\n    tasks = skill.create_tasks(data)\\n    results = skill.run_tasks_in_parallel(tasks)\\n    print(results)\\n\\nFeel free to experiment and figure out if it\\'s useful for your work flow. Her is just some tips:\\n\\nYou can ask anything in the comments below!\\n\\nP.S: Full code ready to be abused available at [https://github.com/Pravko-Solutions/FlashLearn](https://github.com/Pravko-Solutions/FlashLearn)'}\n",
      "{'id': '1ic1mnm', 'title': 'Created an app for practicing for your interviews with GPT', 'author': 'Grapphie', 'subreddit': 'datascience', 'upvotes': 77, 'created_utc': 1738071160.0, 'text': ''}\n",
      "{'id': '1iclhta', 'title': '\"Linear interpolation\" question in interview?', 'author': 'ExoSpectra', 'subreddit': 'datascience', 'upvotes': 14, 'created_utc': 1738124153.0, 'text': 'This may be a bit random/obscure but I have a 30min technical interview on Thursday for a data scientist role, and they said to \"be prepared to solve a linear interpolation problem\". Has anyone seen this before? I know essentially what interpolation is, and looked up what it\\'s used for/how it might show up, but not the most sure here. The role involves forecasting which somewhat relates to interpolation I guess.\\n\\n  \\n(also glassdoor reviews don\\'t mention anything)'}\n",
      "{'id': '1icvhgh', 'title': 'I have open-sourced several of my Data Visualization projects with Plotly', 'author': '-Montse-', 'subreddit': 'datascience', 'upvotes': 84, 'created_utc': 1738162277.0, 'text': ''}\n",
      "{'id': '1id13q6', 'title': 'Data science at FAANG', 'author': 'dev-ai', 'subreddit': 'datascience', 'upvotes': 133, 'created_utc': 1738176118.0, 'text': 'Hi everyone,\\n\\nI created a job board and decided to share here, as I think it can useful. The job board consists of job offers from FAANG companies (Google, Meta, Apple, Amazon, Nvidia, Netflix, Uber, Microsoft, etc.) and allows you to filter job offers by location, years of experience, seniority level, category, etc.\\n\\nYou can check out the \"Data Science\" positions here:\\n\\nhttps://faang.watch/?categories=Data+Science\\n\\nLet me know what you think - feel free to ask questions and request features :)\\n'}\n",
      "{'id': '1id1gn1', 'title': 'Most secure Data Science Jobs?', 'author': 'LebrawnJames416', 'subreddit': 'datascience', 'upvotes': 79, 'created_utc': 1738176995.0, 'text': \"Hey everyone,\\n\\n  \\nI'm constantly hearing news of layoffs and was wondering what areas you think are more secure and how secure do you think your job is?\\n\\n  \\nHow worried are you all about layoffs? Are you always looking for jobs just in case?\"}\n",
      "{'id': '1id8zee', 'title': 'Green AI: Which Programming Language Consumes the Most?', 'author': 'SnooStories6404', 'subreddit': 'datascience', 'upvotes': 0, 'created_utc': 1738196062.0, 'text': ''}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubreddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmissions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselftext\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jhona\\Downloads\\reddit\\venv\\Lib\\site-packages\\praw\\models\\util.py:173\u001b[0m, in \u001b[0;36mstream_generator\u001b[1;34m(function, attribute_name, continue_after_id, exclude_before, pause_after, skip_existing, **function_kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexponential_counter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for post in subreddit.stream.submissions():\n",
    "    payload = {\n",
    "        \"id\": post.id,\n",
    "        \"title\": post.title,\n",
    "        \"author\": str(post.author),\n",
    "        \"subreddit\": post.subreddit.display_name,\n",
    "        \"upvotes\": post.score,\n",
    "        \"created_utc\": post.created_utc,\n",
    "        \"text\": post.selftext\n",
    "    }\n",
    "    print(payload)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
